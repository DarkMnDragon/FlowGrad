{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好地理解 `.vjp`，我们可以通过一个简单的例子来说明这个函数是如何工作的\n",
    "\n",
    "假设我们有一个函数 $ f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $，它将 $ n $ 维输入向量映射到 $ m $ 维输出向量。现在，我们想要计算在某一点 $ \\mathbf{x} $ 的函数输出相对于输入的梯度。为了计算这个梯度，我们可以使用向量-雅可比积，即给定一个输出空间中的向量 $ \\mathbf{v} $，我们想要计算 $ \\mathbf{v} $ 和雅可比矩阵 $ J $ 在点 $ \\mathbf{x} $ 的乘积\n",
    "\n",
    "让我们通过一个具体的例子来看一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function output: tensor([4.0000, 0.8415])\n",
      "Vector-Jacobian Product: tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义一个简单的函数 f(x) = [x[0]^2, sin(x[1])]\n",
    "# 这里 f: R^2 -> R^2\n",
    "def f(x):\n",
    "    return torch.tensor([x[0]**2, torch.sin(x[1])])\n",
    "\n",
    "# 定义输入向量 x\n",
    "x = torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "\n",
    "# 我们想要计算的向量 v，在输出空间中\n",
    "v = torch.tensor([1.0, 0.0])\n",
    "\n",
    "# 使用 vjp 来计算向量-雅可比积\n",
    "output, jacobian_times_v = torch.autograd.functional.vjp(f, x, v)\n",
    "\n",
    "print('Function output:', output)\n",
    "print('Vector-Jacobian Product:', jacobian_times_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，函数 $ f $ 接收一个二维向量 $ \\mathbf{x} $ 并返回另一个二维向量。第一个输出是 $ x_0 $ 的平方，第二个输出是 $ x_1 $ 的正弦值。\n",
    "\n",
    "我们对 $ \\mathbf{x} = [2, 1] $ 处的函数 $ f $ 的输出感兴趣，并且我们想要计算输出向量 $ \\mathbf{v} = [1, 0] $ 与雅可比矩阵的乘积。这意味着我们只关心函数第一个输出相对于输入的梯度。\n",
    "\n",
    "`.vjp` 函数将返回函数 $ f $ 在 $ \\mathbf{x} $ 处的输出和与 $ \\mathbf{v} $ 的向量-雅可比积。在这个例子中，雅可比矩阵 $ J $ 是：\n",
    "\n",
    "$$\n",
    " J =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2x_0 & 0 \\\\\n",
    "0 & \\cos(x_1)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "对于 $ \\mathbf{x} = [2, 1] $，雅可比矩阵 $ J $ 是：\n",
    "\n",
    "$$\n",
    "J =\n",
    "\\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & \\cos(1)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "因此，向量-雅可比积 $ \\mathbf{v}^\\top J $ 将是：\n",
    "\n",
    "$$\n",
    "[1, 0]\n",
    "\\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & \\cos(1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "[4, 0]\n",
    "$$\n",
    "\n",
    "这意味着在 $ \\mathbf{x} = [2, 1] $ 处，向量 $ \\mathbf{v} $ 与雅可比矩阵的乘积仅取决于第一个输出 $ f_0 $ 相对于 $ x_0 $ 的导数，这在我们的例子中是 4。这个导数是由于我们选择了向量 $ \\mathbf{v} $ 为 $[1, 0]$，意味着我们只关心 $ f $ 的第一个分量 $ f_0 $ 相对于 $ x_0 $ 的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习和其他自动微分场景中，通常更倾向于计算向量-雅可比积（VJP）而不是直接计算雅可比矩阵，主要有以下几个原因：\n",
    "\n",
    "1. **计算效率**：对于大型系统，雅可比矩阵可能非常大，直接计算并存储整个雅可比矩阵可能非常消耗资源。VJP 只计算雅可比矩阵和一个向量的乘积，这通常比计算整个雅可比矩阵要快很多，特别是当我们只关心雅可比矩阵与特定向量乘积的结果时。\n",
    "\n",
    "2. **内存效率**：雅可比矩阵的维度与输入和输出的维度有关，对于高维输入和输出，雅可比矩阵可能非常大，而 VJP 避免了存储整个矩阵的需要。\n",
    "\n",
    "3. **实际需求**：在优化问题和梯度下降算法中，我们通常只需要梯度信息，即雅可比矩阵与误差梯度之间的乘积，来更新参数。直接计算 VJP 能够给我们所需的信息，而没有额外的计算负担。\n",
    "\n",
    "4. **链式法则**：自动微分库利用链式法则来高效计算复合函数的导数。在反向传播过程中，梯度（或者说导数信息）是通过每一层传播的，每一步都是一个 VJP 的计算。因此，VJP 的计算是自然嵌入在反向传播中的。\n",
    "\n",
    "5. **高阶导数**：如果我们需要计算高阶导数，即导数的导数，那么使用 VJP 可以很方便地再次应用自动微分规则，而雅可比矩阵就不那么直接了。\n",
    "\n",
    "举一个简单的例子来说明 VJP 的用途：\n",
    "\n",
    "假设我们有一个函数 $ f(\\mathbf{x}) $ 从 $ \\mathbb{R}^n $ 映射到 $ \\mathbb{R}^m $，并且我们想要计算损失函数 $ L = g(f(\\mathbf{x})) $ 相对于 $ \\mathbf{x} $ 的导数，其中 $ g $ 是从 $ \\mathbb{R}^m $ 映射到实数的函数。在这种情况下，我们关心的是 $ \\frac{\\partial L}{\\partial \\mathbf{x}} $，而不是 $ f $ 的雅可比矩阵 $ J_f $ 本身。\n",
    "\n",
    "假设我们知道 $ g $ 相对于 $ f(\\mathbf{x}) $ 的导数 $ \\nabla_{\\mathbf{y}} g $（这里 $ \\mathbf{y} = f(\\mathbf{x}) $），那么我们可以使用 VJP 来直接计算 $ \\frac{\\partial L}{\\partial \\mathbf{x}} $：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}} = J_f^\\top \\nabla_{\\mathbf{y}} g\n",
    "$$\n",
    "\n",
    "其中 $ J_f^\\top$  是 $ f $ 的雅可比矩阵的转置。在这个计算中，我们没有直接计算 $ J_f $；相反，我们直接计算了雅可比矩阵与向量 $ \\nabla_{\\mathbf{y}} g $ 的乘积。这就是 VJP 的用武之地，它在实际应用中比计算整个雅可比矩阵要高效得多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{x_{k/N}} \\mathcal L = (\\nabla_{x_{(k+1)/N}} \\mathcal L) \\cdot J_{\\phi(k)}(x_{k/N}), \\quad \\text{where } \\phi_k(x) = x + \\frac{1}{N} (v_0(x, k/N) + u_{k/N})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "雅可比矩阵 $J_{\\phi_k}(x_{k/N})$ 是函数 $\\phi_k$ 关于其输入 $x_{k/N}$ 在点 $x_{k/N}$ 的一阶偏导数矩阵。它提供了一个线性近似，表示在 $x_{k/N}$ 点附近，系统状态的微小变化如何引起状态转移函数输出的变化。\n",
    "\n",
    "则 $J_{\\phi_k}(x_{k/N})$ 的计算可以表示为：\n",
    "\n",
    "$$\n",
    "J_{\\phi_k}(x_{k/N}) = \\frac{\\partial \\phi_k}{\\partial x_{k/N}}\\bigg|_{x_{k/N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雅可比矩阵（Jacobi Matrix），在多变量微积分中，是一个函数向量对其输入向量的所有一阶偏导数的矩阵。简单来说，如果你有一个由多个函数组成的向量函数，这些函数又都依赖于多个变量，那么雅可比矩阵就包含了这些函数相对于每个变量的偏导数。\n",
    "\n",
    "### 定义\n",
    "\n",
    "考虑一个向量值函数 $\\mathbf{F} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$，它将一个 $n$-维向量 $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)^\\top$ 映射到一个 $m$-维向量 $\\mathbf{y} = (y_1, y_2, \\ldots, y_m)^\\top$。函数 $\\mathbf{F}$ 可以表示为：\n",
    "\n",
    "$$\n",
    "\\mathbf{F}(\\mathbf{x}) = \n",
    "\\begin{pmatrix}\n",
    "f_1(x_1, x_2, \\ldots, x_n) \\\\\n",
    "f_2(x_1, x_2, \\ldots, x_n) \\\\\n",
    "\\vdots \\\\\n",
    "f_m(x_1, x_2, \\ldots, x_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "其中，每个 $f_i$ 是一个标量函数。\n",
    "\n",
    "雅可比矩阵 $\\mathbf{J_F}(\\mathbf{x})$ 定义为：\n",
    "\n",
    "$$\n",
    "\\mathbf{J_F}(\\mathbf{x}) = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### 举例\n",
    "\n",
    "假设有一个向量值函数 $\\mathbf{F} : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$，由下面两个标量函数组成：\n",
    "\n",
    "$$\n",
    "f_1(x, y) = x^2 + y^2\n",
    "$$\n",
    "$$\n",
    "f_2(x, y) = 3x + 4y\n",
    "$$\n",
    "\n",
    "那么，$\\mathbf{F}$ 的雅可比矩阵 $\\mathbf{J_F}(x, y)$ 将是：\n",
    "\n",
    "$$\n",
    "\\mathbf{J_F}(x, y) = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2x & 2y \\\\\n",
    "3 & 4\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "在这个例子中，雅可比矩阵 $\\mathbf{J_F}(x, y)$ 提供了一个线性近似，描述了函数 $\\mathbf{F}$ 输出的微小变化是如何响应输入 $(x, y)$ 的微小变化的。这种性质在优化、动态系统分析和机器学习模型的训练中尤为重要，因为它允许我们使用线性方法来近似和分析本质上是非线性的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update \n",
    "更多的例子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix J:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义函数 f(x) = ReLU(Ax + b)\n",
    "def f(x):\n",
    "    A = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = torch.tensor([1.0, 2.0])\n",
    "    return torch.relu(torch.matmul(A, x) + b)\n",
    "\n",
    "# 输入向量 x\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# 应用函数\n",
    "y = f(x)\n",
    "\n",
    "# 初始化 Jacobian 矩阵为零矩阵\n",
    "J = torch.zeros((len(y), len(x)))\n",
    "\n",
    "# 计算 Jacobian\n",
    "for i in range(len(y)):\n",
    "    # 清除之前的梯度\n",
    "    if x.grad is not None:\n",
    "        x.grad.data.zero_()\n",
    "    y[i].backward(retain_graph=True)\n",
    "    J[i] = x.grad.clone()\n",
    "\n",
    "print(\"Jacobian matrix J:\\n\", J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8707, -6.8236], grad_fn=<AddBackward0>)\n",
      "Jacobian matrix J:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "output &  VJP:\n",
      " tensor([-1.8707, -6.8236]) tensor([3.0000, 4.5000, 6.0000])\n"
     ]
    }
   ],
   "source": [
    "# 定义函数 f(x) = ReLU(Ax + b)\n",
    "def f(x):\n",
    "    A = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=False)\n",
    "    b = torch.tensor([1.0, 2.0], requires_grad=False)\n",
    "    return torch.matmul(A, x) + b\n",
    "\n",
    "# 输入向量 x\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(f(x))\n",
    "\n",
    "# 使用torch.autograd.functional.jacobian直接计算Jacobian\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd.functional import vjp\n",
    "\n",
    "J = jacobian(f, x)\n",
    "\n",
    "print(\"Jacobian matrix J:\\n\", J)\n",
    "\n",
    "output, VJP = vjp(f, x, torch.tensor([1, 0.5]))\n",
    "\n",
    "print(\"output &  VJP:\\n\", output, VJP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0444, 0.7225, 0.0613, 5.2584],\n",
      "        [0.2219, 1.2179, 0.2964, 0.0305],\n",
      "        [1.1483, 2.3830, 0.2691, 4.1694]])\n",
      "tensor([[ 6.2584,  1.2499, -0.6908,  2.0454],\n",
      "        [ 2.3056, -3.1897, -0.6932, -0.1136],\n",
      "        [-0.8251, -2.5213, -0.5486,  4.9091]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd.functional import vjp\n",
    "\n",
    "def f(x):\n",
    "    # 示例函数，x 是一个多维张量\n",
    "    return x ** 2\n",
    "\n",
    "x = torch.randn(3, 4, requires_grad=True)  # 多维输入张量\n",
    "v = torch.randn(3, 4)  # 与 f(x) 输出形状相同的张量\n",
    "\n",
    "# 计算 vjp\n",
    "output, jacobian_v = vjp(f, x, v)\n",
    "print(output)\n",
    "print(jacobian_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
